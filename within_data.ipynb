{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "within_data.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPL8Pe6xVIYpzuBQS/GHot+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/achrisk/Dissertation/blob/main/within_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTJZmowrd5sX",
        "outputId": "d04afbbb-5109-43d8-b11a-9caea7ce020b"
      },
      "source": [
        "!git clone https://github.com/achrisk/Dissertation.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Dissertation'...\n",
            "remote: Enumerating objects: 62, done.\u001b[K\n",
            "remote: Counting objects: 100% (62/62), done.\u001b[K\n",
            "remote: Compressing objects: 100% (60/60), done.\u001b[K\n",
            "remote: Total 62 (delta 19), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (62/62), done.\n",
            "Checking out files: 100% (27/27), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ggxmna_d9u8"
      },
      "source": [
        "import scipy.io as spio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from importlib.machinery import SourceFileLoader\n",
        "import pickle\n",
        "\n",
        "# EEGNet-specific imports\n",
        "from Dissertation.EEGModels import EEGNet\n",
        "from tensorflow.keras import utils as np_utils\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Activation, Permute, Dropout\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
        "from tensorflow.keras.layers import SeparableConv2D, DepthwiseConv2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Input, Flatten\n",
        "from tensorflow.keras.constraints import max_norm#\n",
        "from google.colab import files\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "\n",
        "from tensorflow.keras.layers import SpatialDropout2D\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# tools for plotting confusion matrices\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.signal import butter, lfilter"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEGRbemoeAzt"
      },
      "source": [
        "# Band-pass Filter\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmOFYNBGwRzj"
      },
      "source": [
        "def DeepConvNet(nb_classes, Chans = 64, Samples = 256,\r\n",
        "                dropoutRate = 0.5):\r\n",
        "    \"\"\" Keras implementation of the Deep Convolutional Network as described in\r\n",
        "    Schirrmeister et. al. (2017), Human Brain Mapping.\r\n",
        "    \r\n",
        "    This implementation assumes the input is a 2-second EEG signal sampled at \r\n",
        "    128Hz, as opposed to signals sampled at 250Hz as described in the original\r\n",
        "    paper. We also perform temporal convolutions of length (1, 5) as opposed\r\n",
        "    to (1, 10) due to this sampling rate difference. \r\n",
        "    \r\n",
        "    Note that we use the max_norm constraint on all convolutional layers, as \r\n",
        "    well as the classification layer. We also change the defaults for the\r\n",
        "    BatchNormalization layer. We used this based on a personal communication \r\n",
        "    with the original authors.\r\n",
        "    \r\n",
        "                      ours        original paper\r\n",
        "    pool_size        1, 2        1, 3\r\n",
        "    strides          1, 2        1, 3\r\n",
        "    conv filters     1, 5        1, 10\r\n",
        "    \r\n",
        "    Note that this implementation has not been verified by the original \r\n",
        "    authors. \r\n",
        "    \r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # start the model\r\n",
        "    input_main   = Input((1, Chans, Samples))\r\n",
        "    block1       = Conv2D(25, (1, 10), \r\n",
        "                                 input_shape=(1, Chans, Samples),\r\n",
        "                                 kernel_constraint = max_norm(2., axis=(0,1,2)))(input_main)\r\n",
        "    block1       = Conv2D(25, (Chans, 1),\r\n",
        "                                 kernel_constraint = max_norm(2., axis=(0,1,2)))(block1)\r\n",
        "    block1       = BatchNormalization(axis=1, epsilon=1e-05, momentum=0.1)(block1)\r\n",
        "    block1       = Activation('elu')(block1)\r\n",
        "    block1       = MaxPooling2D(pool_size=(1, 2), strides=(1, 2))(block1)\r\n",
        "    block1       = Dropout(dropoutRate)(block1)\r\n",
        "  \r\n",
        "    block2       = Conv2D(50, (1, 10),\r\n",
        "                                 kernel_constraint = max_norm(2., axis=(0,1,2)))(block1)\r\n",
        "    block2       = BatchNormalization(axis=1, epsilon=1e-05, momentum=0.1)(block2)\r\n",
        "    block2       = Activation('elu')(block2)\r\n",
        "    block2       = MaxPooling2D(pool_size=(1, 2), strides=(1, 2))(block2)\r\n",
        "    block2       = Dropout(dropoutRate)(block2)\r\n",
        "    \r\n",
        "    block3       = Conv2D(100, (1, 10),\r\n",
        "                                 kernel_constraint = max_norm(2., axis=(0,1,2)))(block2)\r\n",
        "    block3       = BatchNormalization(axis=1, epsilon=1e-05, momentum=0.1)(block3)\r\n",
        "    block3       = Activation('elu')(block3)\r\n",
        "    block3       = MaxPooling2D(pool_size=(1, 2), strides=(1, 2))(block3)\r\n",
        "    block3       = Dropout(dropoutRate)(block3)\r\n",
        "    \r\n",
        "    block4       = Conv2D(200, (1, 10),\r\n",
        "                                 kernel_constraint = max_norm(2., axis=(0,1,2)))(block3)\r\n",
        "    block4       = BatchNormalization(axis=1, epsilon=1e-05, momentum=0.1)(block4)\r\n",
        "    block4       = Activation('elu')(block4)\r\n",
        "    block4       = MaxPooling2D(pool_size=(1, 2), strides=(1, 2))(block4)\r\n",
        "    block4       = Dropout(dropoutRate)(block4)\r\n",
        "    \r\n",
        "    flatten      = Flatten()(block4)\r\n",
        "    \r\n",
        "    dense        = Dense(nb_classes, kernel_constraint = max_norm(0.5))(flatten)\r\n",
        "    softmax      = Activation('softmax')(dense)\r\n",
        "    \r\n",
        "    return Model(inputs=input_main, outputs=softmax)\r\n",
        "\r\n",
        "\r\n",
        "# need these for ShallowConvNet\r\n",
        "def square(x):\r\n",
        "    return K.square(x)\r\n",
        "\r\n",
        "def log(x):\r\n",
        "    return K.log(K.clip(x, min_value = 1e-7, max_value = 10000)) \r\n",
        "\r\n",
        "def ShallowConvNet(nb_classes, Chans = 64, Samples = 128, dropoutRate = 0.5):\r\n",
        "    \"\"\" Keras implementation of the Shallow Convolutional Network as described\r\n",
        "    in Schirrmeister et. al. (2017), Human Brain Mapping.\r\n",
        "    \r\n",
        "    Assumes the input is a 2-second EEG signal sampled at 128Hz. Note that in \r\n",
        "    the original paper, they do temporal convolutions of length 25 for EEG\r\n",
        "    data sampled at 250Hz. We instead use length 13 since the sampling rate is \r\n",
        "    roughly half of the 250Hz which the paper used. The pool_size and stride\r\n",
        "    in later layers is also approximately half of what is used in the paper.\r\n",
        "    \r\n",
        "    Note that we use the max_norm constraint on all convolutional layers, as \r\n",
        "    well as the classification layer. We also change the defaults for the\r\n",
        "    BatchNormalization layer. We used this based on a personal communication \r\n",
        "    with the original authors.\r\n",
        "    \r\n",
        "                     ours        original paper\r\n",
        "    pool_size        1, 35       1, 75\r\n",
        "    strides          1, 7        1, 15\r\n",
        "    conv filters     1, 13       1, 25    \r\n",
        "    \r\n",
        "    Note that this implementation has not been verified by the original \r\n",
        "    authors. We do note that this implementation reproduces the results in the\r\n",
        "    original paper with minor deviations. \r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # start the model\r\n",
        "    input_main   = Input((1, Chans, Samples))\r\n",
        "    block1       = Conv2D(40, (1, 13), \r\n",
        "                                 input_shape=(1, Chans, Samples),\r\n",
        "                                 kernel_constraint = max_norm(2., axis=(0,1,2)))(input_main)\r\n",
        "    block1       = Conv2D(40, (Chans, 1), use_bias=False, \r\n",
        "                          kernel_constraint = max_norm(2., axis=(0,1,2)))(block1)\r\n",
        "    block1       = BatchNormalization(axis=1, epsilon=1e-05, momentum=0.1)(block1)\r\n",
        "    block1       = Activation(square)(block1)\r\n",
        "    block1       = AveragePooling2D(pool_size=(1, 35), strides=(1, 7))(block1)\r\n",
        "    block1       = Activation(log)(block1)\r\n",
        "    block1       = Dropout(dropoutRate)(block1)\r\n",
        "    flatten      = Flatten()(block1)\r\n",
        "    dense        = Dense(nb_classes, kernel_constraint = max_norm(0.5))(flatten)\r\n",
        "    softmax      = Activation('softmax')(dense)\r\n",
        "    \r\n",
        "    return Model(inputs=input_main, outputs=softmax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkTcjdRVeA2j"
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "X_tr = np.empty([80, 12, 4096])\n",
        "result=[]\n",
        "\n",
        "h_cut = [30] # [only alpha, alpha + low beta, alpha + beta] 18, 24, 30\n",
        "drop_out = [0.25] # 0.25, 0.5\n",
        "k_len = [64] # 32, 64, 128, 256\n",
        "n_epochs = 500\n",
        "model_name = 'EEGNet'\n",
        "kernels, chans, samples = 1, 12, 1536\n",
        "\n",
        "outfname = 'accuray_epochs' + str(n_epochs) + '_k_len_' + str(k_len) + '_filter_' + str(h_cut) + '_drop_' + str(drop_out) + '_model_' + str(model_name) + '_patient_data.pickle'\n",
        "outfname_history = 'accuray_epochs' + str(n_epochs) + '_k_len_' + str(k_len) + '_filter_' + str(h_cut) + '_drop_' + str(drop_out) + '_model_' + str(model_name) + '_history_data.pickle'\n",
        "\n",
        "\n",
        "nsub = 8\n",
        "nfilt = len(h_cut)\n",
        "ndrop = len(drop_out)\n",
        "nkl = len(k_len)\n",
        "acc_sub = np.zeros([nsub, nfilt,ndrop,nkl])\n",
        "hyper_parameter_matrix = np.zeros([nsub, nfilt, ndrop, nkl, 5])\n",
        "history = {}\n",
        "\n",
        "kappa_statistic_per_patient = {}\n",
        "kappa_scores= []\n",
        "accuracy_scores= []\n",
        "\n",
        "\n",
        "\n",
        "for sub_idx, x in enumerate(range(1,nsub+1)):\n",
        "    for h_indx, h in enumerate(h_cut):\n",
        "        fName = 'Dissertation/EEG data/parsed_P0' + str(x) + 'T.mat'  # Load Data\n",
        "        print(fName)\n",
        "        mat = spio.loadmat(fName)\n",
        "        r_X_tr = mat['RawEEGData']\n",
        "        y_tr = mat['Labels']\n",
        "        y_tr = y_tr.flatten() \n",
        "\n",
        "        print(np.shape(r_X_tr))\n",
        "        print(np.shape(y_tr))\n",
        "\n",
        "        for t in range(r_X_tr.shape[0]):\n",
        "            tril = r_X_tr[t,:,:]\n",
        "            #tril = tril.transpose()\n",
        "            tril_filtered = butter_bandpass_filter(tril, lowcut=8, highcut=h, fs=512, order=4)\n",
        "            # tril_filtered = tril_filtered.transpose()\n",
        "            X_tr[t,:,:] = tril_filtered \n",
        "\n",
        "        indices = np.random.permutation(X_tr.shape[0])\n",
        "        X_tr = X_tr[indices, :, :]\n",
        "        y_tr = y_tr[indices]\n",
        "        # split data of each subject in training and validation\n",
        "        X_train = X_tr[0:40,:,2048:3584]\n",
        "        Y_train = y_tr[0:40]\n",
        "        X_val = X_tr[40:60,:,2048:3584]\n",
        "        Y_val = y_tr[40:60]\n",
        "        X_test = X_tr[60:,:,2048:3584]\n",
        "        Y_test = y_tr[60:]\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "        print(np.shape(X_train))\n",
        "        print(np.shape(Y_train))\n",
        "        print(np.shape(X_val))\n",
        "        print(np.shape(Y_val))\n",
        "        print(np.shape(X_test))\n",
        "        print(np.shape(Y_test))\n",
        "\n",
        "        # convert labels to one-hot encodings.\n",
        "        Y_train = np_utils.to_categorical(Y_train-1, num_classes=2)\n",
        "        Y_val = np_utils.to_categorical(Y_val-1, num_classes=2)\n",
        "        Y_test = np_utils.to_categorical(Y_test-1, num_classes=2)\n",
        "\n",
        "        # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "        # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "        X_train = X_train.reshape(X_train.shape[0], kernels, chans, samples)\n",
        "        X_val = X_val.reshape(X_val.shape[0], kernels, chans, samples)\n",
        "        X_test = X_test.reshape(X_test.shape[0], kernels, chans, samples)\n",
        "\n",
        "       \n",
        "\n",
        "        print('X_train shape:', X_train.shape)\n",
        "        print(X_train.shape[0], 'train samples')\n",
        "        print(X_val.shape[0], 'val samples')\n",
        "\n",
        "        print('X_train shape:', X_test.shape)\n",
        "        print(X_test.shape[0], 'test samples')\n",
        "        plt.figure(figsize=(20,20))\n",
        "        \n",
        "\n",
        "        #for i in range(np.shape(X_train)[2]):\n",
        "          #plt.plot(X_train[0,:,i,:][0])\n",
        "        #plt.show()\n",
        "       \n",
        "\n",
        "        for id_d, d in enumerate(drop_out):\n",
        "            for id_kl, kl in enumerate(k_len):\n",
        "              print(id_kl, id_d)\n",
        "              # configure the EEGNet-8,2,16 model with kernel length of 32 samples (other \n",
        "              # model configurations may do better, but this is a good starting point)\n",
        "              model = EEGNet(nb_classes = 2, Chans = chans, Samples = samples,\n",
        "                             dropoutRate = d, kernLength = kl, F1 = 8,D = 2, F2 = 16,\n",
        "                             norm_rate = 0.25, dropoutType = 'Dropout')\n",
        "              #model.add_metric(tfa.metrics.CohenKappa(num_classes=2), name = 'kappa') #remove later\n",
        "\n",
        "              # compile the model and set the optimizers\n",
        "              model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
        "                            metrics = ['accuracy'])\n",
        "\n",
        "              # count number of parameters in the model\n",
        "              numParams    = model.count_params() \n",
        "\n",
        "              # set a valid path for your system to record model checkpoints\n",
        "              checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=1, save_best_only=True)\n",
        "\n",
        "              # the syntax is {class_1:weight_1, class_2:weight_2,...}. Here just setting\n",
        "              # the weights all to be 1\n",
        "              class_weights = {0:1, 1:1}\n",
        "\n",
        "              history_temp = model.fit(X_train, Y_train, batch_size = 16, epochs = n_epochs, \n",
        "                              verbose = 2, validation_data=(X_val, Y_val),\n",
        "                              callbacks=[checkpointer], class_weight = class_weights)\n",
        "          \n",
        "              history[sub_idx,h_indx,id_d] = history_temp.history\n",
        "\n",
        "              print('\\n# Evaluate on test data')\n",
        "\n",
        "\n",
        "              results = model.evaluate(X_test, Y_test)\n",
        "              print('test loss, test acc:', results)\n",
        "\n",
        "\n",
        "              predictions = model.predict(x=X_test)\n",
        "              assert predictions.shape == Y_test.shape\n",
        "              kappa_statistic_per_patient[sub_idx,h_indx,id_d, id_kl] = cohen_kappa_score(np.argmax(Y_test, axis=1), np.argmax(predictions, axis=1))\n",
        "              print (kappa_statistic_per_patient)\n",
        "              \n",
        "             \n",
        "              \n",
        "          \n",
        "\n",
        "\n",
        "\n",
        "              print(results)\n",
        "              acc_sub[sub_idx,h_indx,id_d, id_kl] = results[1]\n",
        "              hyper_parameter_matrix[sub_idx,h_indx,id_d, id_kl, :] = [x, h, d, kl, results[1]]\n",
        "\n",
        "              accuracy_scores.append(results[1])\n",
        "              #print (accuracy_scores)\n",
        "\n",
        "\n",
        "              K.clear_session()\n",
        "              del checkpointer, model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for key in kappa_statistic_per_patient:\n",
        "  \n",
        "  kappa_scores.append(kappa_statistic_per_patient[key])\n",
        "  \n",
        "\n",
        "plt.style.use('ggplot')\n",
        " \n",
        "x_pos = ( 1, 2, 3, 4, 5, 6, 7, 8)\n",
        "\n",
        "plt.bar(x_pos, kappa_scores, color='green')\n",
        "plt.xlabel(\"Subjects\")\n",
        "plt.ylabel(\"Kappa Score\")\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "plt.bar(x_pos, accuracy_scores, color ='green')\n",
        "plt.xlabel(\"Subjects\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open(outfname, 'wb') as handle:\n",
        "    pickle.dump(hyper_parameter_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)         \n",
        "with open(outfname_history, 'wb') as handle:\n",
        "    pickle.dump(history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOqoOs--eA4-"
      },
      "source": [
        "plt.figure(figsize=(20,20))\r\n",
        "for i in range(np.shape(X_train)[2]):\r\n",
        "  plt.plot(X_train[0,:,i,:][0])\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo9a7Ds3eBAY"
      },
      "source": [
        "from google.colab import files\r\n",
        "\r\n",
        "h_cut = [30] # [only alpha, alpha + low beta, alpha + beta]\r\n",
        "drop_out = [0.25]\r\n",
        "n_epochs = 500\r\n",
        "k_len = [64]\r\n",
        "model_name = 'EEGNet'  # ShallowConvNet or DeepConvNet\r\n",
        "kernels, chans, samples = 1, 12, 1536\r\n",
        "subject = 1\r\n",
        "outfname_history = 'accuray_epochs' + str(n_epochs) + '_k_len_' + str(k_len) + '_filter_' + str(h_cut) + '_drop_' + str(drop_out) + '_model_' + str(model_name) + '_history_data.pickle'\r\n",
        "\r\n",
        "with open(outfname_history, 'rb') as handle:\r\n",
        "    history = pickle.load(handle)\r\n",
        "\r\n",
        "#for key in history.keys():\r\n",
        "#  plt.figure()\r\n",
        "#  history_ = history[key]\r\n",
        "#  plt.plot(history_['accuracy'])\r\n",
        "#  plt.plot(history_['val_accuracy'])\r\n",
        "#  plt.ylabel('accuracy')\r\n",
        "#  plt.xlabel('epoch no.')\r\n",
        "#  plt.legend(['train', 'valid'], loc='upper left')\r\n",
        "#  plt.show()\r\n",
        "#  plt.savefig('accuracy_epochs' + str(n_epochs) +  '_k_len_' + str(k_len)  '_filter_' + str(h_cut) + '_drop_' + str(drop_out) + '_model_' + str(model_name) + 'subject '+str(subject)+'.jpeg')\r\n",
        "  #files.download('accuracy_epochs' + str(n_epochs) + '_k_len_' + str(k_len)  '_filter_' + str(h_cut) + '_drop_' + str(drop_out) + '_model_' + str(model_name) + 'subject '+str(subject)+'.jpeg')\r\n",
        "#  subject += 1\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "vx2JKRj0xVWI",
        "outputId": "232b0318-392f-4e67-d9ff-ecf4ac351a45"
      },
      "source": [
        "  files.download('accuracy_epochs500_filter_[30]_drop_[0.25]_model_EEGNetsubject:1.jpeg')\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_dbf87d17-c4d4-4918-81bc-53fdefd80cb3\", \"accuracy_epochs500_filter_[30]_drop_[0.25]_model_EEGNetsubject:1.jpeg\", 2571)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1yqGuhWvlin",
        "outputId": "4b054539-0477-4ceb-ab84-f06b7287a9aa"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'accuracy_epochs500_filter_[30]_drop_[0.25]_model_EEGNetsubject:1.jpeg'\n",
            "'accuracy_epochs500_filter_[30]_drop_[0.25]_model_EEGNetsubject:2.jpeg'\n",
            "'accuracy_epochs500_filter_[30]_drop_[0.25]_model_EEGNetsubject:3.jpeg'\n",
            "'accuracy_epochs500_filter_[30]_drop_[0.25]_model_EEGNetsubject:4.jpeg'\n",
            "'accuracy_epochs500_filter_[30]_drop_[0.25]_model_EEGNetsubject:5.jpeg'\n",
            "'accuracy_epochs500_filter_[30]_drop_[0.25]_model_EEGNetsubject:6.jpeg'\n",
            "'accuracy_epochs500_filter_[30]_drop_[0.25]_model_EEGNetsubject:7.jpeg'\n",
            "'accuracy_epochs500_filter_[30]_drop_[0.25]_model_EEGNetsubject:8.jpeg'\n",
            "'accuracy_epochs500_filter_[30]_drop_[0.25]_model_EEGNetsubject:.jpeg'\n",
            "'accuracy_epochs500_filter_[30]_drop_[0.25]_model_EEGNetsubject.jpeg'\n",
            "'accuray_epochs500_filter_[30]_drop_[0.25]_model_DeepConvNet_history_data.pickle'\n",
            "'accuray_epochs500_filter_[30]_drop_[0.25]_model_DeepConvNet_patient_data.pickle'\n",
            "'accuray_epochs500_filter_[30]_drop_[0.25]_model_EEGNet_history_data.pickle'\n",
            "'accuray_epochs500_filter_[30]_drop_[0.25]_model_EEGNet_patient_data.pickle'\n",
            " \u001b[0m\u001b[01;34mDissertation\u001b[0m/\n",
            " \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D49hSgEkeA9_"
      },
      "source": [
        "K.clear_session()\r\n",
        "\r\n",
        "X_tr = np.empty([80, 12, 4096])\r\n",
        "result=[]\r\n",
        "\r\n",
        "h_cut = [30] # [only alpha, alpha + low beta, alpha + beta]\r\n",
        "drop_out = [0.25]\r\n",
        "n_epochs = 500\r\n",
        "model_name = 'DeepConvNet'  # ShallowConvNet or DeepConvNet\r\n",
        "kernels, chans, samples = 1, 12, 1536\r\n",
        "\r\n",
        "outfname = 'accuray_epochs' + str(n_epochs) + '_filter_' + str(h_cut) + '_drop_' + str(drop_out) + '_model_' + str(model_name) + '_patient_data.pickle'\r\n",
        "outfname_history = 'accuray_epochs' + str(n_epochs) + '_filter_' + str(h_cut) + '_drop_' + str(drop_out) + '_model_' + str(model_name) + '_history_data.pickle'\r\n",
        "\r\n",
        "nsub = 8\r\n",
        "nfilt = len(h_cut)\r\n",
        "ndrop = len(drop_out)\r\n",
        "acc_sub = np.zeros([nsub, nfilt,ndrop])\r\n",
        "hyper_parameter_matrix = np.zeros([nsub, nfilt, ndrop, 4])\r\n",
        "history={}\r\n",
        "\r\n",
        "for sub_idx, x in enumerate(range(1,nsub+1)):\r\n",
        "    for h_indx, h in enumerate(h_cut):\r\n",
        "        fName = 'Dissertation/EEG data/parsed_P0' + str(x) + 'T.mat'  # Load Data\r\n",
        "        print(fName)\r\n",
        "        mat = spio.loadmat(fName)\r\n",
        "        r_X_tr = mat['RawEEGData']\r\n",
        "        y_tr = mat['Labels']\r\n",
        "        y_tr = y_tr.flatten() \r\n",
        "\r\n",
        "        print(np.shape(r_X_tr))\r\n",
        "        print(np.shape(y_tr))\r\n",
        "\r\n",
        "        for t in range(r_X_tr.shape[0]):\r\n",
        "            tril = r_X_tr[t,:,:]\r\n",
        "            #tril = tril.transpose()\r\n",
        "            tril_filtered = butter_bandpass_filter(tril, lowcut=8, highcut=h, fs=512, order=4)\r\n",
        "            # tril_filtered = tril_filtered.transpose()\r\n",
        "            X_tr[t,:,:] = tril_filtered \r\n",
        "\r\n",
        "        # split data of each subject in training and validation\r\n",
        "        X_train = X_tr[0:50,:,2048:3584]\r\n",
        "        Y_train = y_tr[0:50]\r\n",
        "        X_val = X_tr[50:70,:,2048:3584]\r\n",
        "        Y_val = y_tr[50:70]\r\n",
        "        X_test = X_tr[70:,:,2048:3584]\r\n",
        "        Y_test = y_tr[70:]\r\n",
        "\r\n",
        "        print(np.shape(X_train))\r\n",
        "        print(np.shape(Y_train))\r\n",
        "        print(np.shape(X_val))\r\n",
        "        print(np.shape(Y_val))\r\n",
        "        print(np.shape(X_test))\r\n",
        "        print(np.shape(Y_test))\r\n",
        "\r\n",
        "        # convert labels to one-hot encodings.\r\n",
        "        Y_train = np_utils.to_categorical(Y_train-1, num_classes=2)\r\n",
        "        Y_val = np_utils.to_categorical(Y_val-1, num_classes=2)\r\n",
        "        Y_test = np_utils.to_categorical(Y_test-1, num_classes=2)\r\n",
        "\r\n",
        "        # convert data to NCHW (trials, kernels, channels, samples) format. Data \r\n",
        "        # contains 22 channels and 500 time-points. Set the number of kernels to 1.\r\n",
        "        X_train = X_train.reshape(X_train.shape[0], kernels, chans, samples)\r\n",
        "        X_val = X_val.reshape(X_val.shape[0], kernels, chans, samples)\r\n",
        "        X_test = X_test.reshape(X_test.shape[0], kernels, chans, samples)\r\n",
        "\r\n",
        "        print('X_train shape:', X_train.shape)\r\n",
        "        print(X_train.shape[0], 'train samples')\r\n",
        "        print(X_val.shape[0], 'val samples')\r\n",
        "\r\n",
        "        print('X_train shape:', X_test.shape)\r\n",
        "        print(X_test.shape[0], 'test samples')\r\n",
        "\r\n",
        "        for id_d, d in enumerate(drop_out):\r\n",
        "          print(id_d)\r\n",
        "          \r\n",
        "          if model_name == 'DeepConvNet':\r\n",
        "            model = DeepConvNet(nb_classes=2, Chans = chans, Samples = samples, dropoutRate = d)\r\n",
        "          elif model_name == 'ShallowConvNet':\r\n",
        "            model = ShallowConvNet(nb_classes = 2, Chans = chans, Samples = samples, dropoutRate = d)\r\n",
        "          else:\r\n",
        "            raise ValueError('Model name wrong!')\r\n",
        "          \r\n",
        "\r\n",
        "          # compile the model and set the optimizers\r\n",
        "          model.compile(loss='categorical_crossentropy', optimizer='adam', \r\n",
        "                        metrics = ['accuracy'])\r\n",
        "\r\n",
        "          # count number of parameters in the model\r\n",
        "          numParams    = model.count_params() \r\n",
        "\r\n",
        "          # set a valid path for your system to record model checkpoints\r\n",
        "          checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=1, save_best_only=True)\r\n",
        "\r\n",
        "          # the syntax is {class_1:weight_1, class_2:weight_2,...}. Here just setting\r\n",
        "          # the weights all to be 1\r\n",
        "          class_weights = {0:1, 1:1}\r\n",
        "\r\n",
        "          history_temp = model.fit(X_train, Y_train, batch_size = 16, epochs = n_epochs, \r\n",
        "                              verbose = 2, validation_data=(X_val, Y_val),\r\n",
        "                              callbacks=[checkpointer], class_weight = class_weights)\r\n",
        "          \r\n",
        "          history[sub_idx,h_indx,id_d] = history_temp.history\r\n",
        "\r\n",
        "          print('\\n# Evaluate on test data')\r\n",
        "\r\n",
        "\r\n",
        "          results = model.evaluate(X_test, Y_test)\r\n",
        "          print('test loss, test acc:', results)\r\n",
        "\r\n",
        "          print(results)\r\n",
        "          acc_sub[sub_idx,h_indx,id_d] = results[1]\r\n",
        "          hyper_parameter_matrix[sub_idx,h_indx,id_d, :] = [x, h, d, results[1]]\r\n",
        "\r\n",
        "          K.clear_session()\r\n",
        "with open(outfname, 'wb') as handle:\r\n",
        "    pickle.dump(hyper_parameter_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)         \r\n",
        "with open(outfname_history, 'wb') as handle:\r\n",
        "    pickle.dump(history, handle, protocol=pickle.HIGHEST_PROTOCOL)      \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsMiBBYIeA8F"
      },
      "source": [
        "K.clear_session()\r\n",
        "\r\n",
        "X_tr = np.empty([80, 12, 4096])\r\n",
        "result=[]\r\n",
        "\r\n",
        "h_cut = [30] # [only alpha, alpha + low beta, alpha + beta]\r\n",
        "drop_out = [0.25]\r\n",
        "n_epochs = 500\r\n",
        "model_name = 'DeepConvNet'  # ShallowConvNet or DeepConvNet\r\n",
        "kernels, chans, samples = 1, 12, 1536\r\n",
        "\r\n",
        "outfname = 'accuray_epochs' + str(n_epochs) + '_filter_' + str(h_cut) + '_drop_' + str(drop_out) + '_model_' + str(model_name) + '_patient_data.pickle'\r\n",
        "outfname_history = 'accuray_epochs' + str(n_epochs) + '_filter_' + str(h_cut) + '_drop_' + str(drop_out) + '_model_' + str(model_name) + '_history_data.pickle'\r\n",
        "\r\n",
        "nsub = 8\r\n",
        "nfilt = len(h_cut)\r\n",
        "ndrop = len(drop_out)\r\n",
        "acc_sub = np.zeros([nsub, nfilt,ndrop])\r\n",
        "hyper_parameter_matrix = np.zeros([nsub, nfilt, ndrop, 4])\r\n",
        "history={}\r\n",
        "\r\n",
        "for sub_idx, x in enumerate(range(1,nsub+1)):\r\n",
        "    for h_indx, h in enumerate(h_cut):\r\n",
        "        fName = 'Dissertation/EEG data/parsed_P0' + str(x) + 'T.mat'  # Load Data\r\n",
        "        print(fName)\r\n",
        "        mat = spio.loadmat(fName)\r\n",
        "        r_X_tr = mat['RawEEGData']\r\n",
        "        y_tr = mat['Labels']\r\n",
        "        y_tr = y_tr.flatten() \r\n",
        "\r\n",
        "        print(np.shape(r_X_tr))\r\n",
        "        print(np.shape(y_tr))\r\n",
        "\r\n",
        "        for t in range(r_X_tr.shape[0]):\r\n",
        "            tril = r_X_tr[t,:,:]\r\n",
        "            #tril = tril.transpose()\r\n",
        "            tril_filtered = butter_bandpass_filter(tril, lowcut=8, highcut=h, fs=512, order=4)\r\n",
        "            # tril_filtered = tril_filtered.transpose()\r\n",
        "            X_tr[t,:,:] = tril_filtered \r\n",
        "\r\n",
        "        # split data of each subject in training and validation\r\n",
        "        X_train = X_tr[0:50,:,2048:3584]\r\n",
        "        Y_train = y_tr[0:50]\r\n",
        "        X_val = X_tr[50:70,:,2048:3584]\r\n",
        "        Y_val = y_tr[50:70]\r\n",
        "        X_test = X_tr[70:,:,2048:3584]\r\n",
        "        Y_test = y_tr[70:]\r\n",
        "\r\n",
        "        print(np.shape(X_train))\r\n",
        "        print(np.shape(Y_train))\r\n",
        "        print(np.shape(X_val))\r\n",
        "        print(np.shape(Y_val))\r\n",
        "        print(np.shape(X_test))\r\n",
        "        print(np.shape(Y_test))\r\n",
        "\r\n",
        "        # convert labels to one-hot encodings.\r\n",
        "        Y_train = np_utils.to_categorical(Y_train-1, num_classes=2)\r\n",
        "        Y_val = np_utils.to_categorical(Y_val-1, num_classes=2)\r\n",
        "        Y_test = np_utils.to_categorical(Y_test-1, num_classes=2)\r\n",
        "\r\n",
        "        # convert data to NCHW (trials, kernels, channels, samples) format. Data \r\n",
        "        # contains 22 channels and 500 time-points. Set the number of kernels to 1.\r\n",
        "        X_train = X_train.reshape(X_train.shape[0], kernels, chans, samples)\r\n",
        "        X_val = X_val.reshape(X_val.shape[0], kernels, chans, samples)\r\n",
        "        X_test = X_test.reshape(X_test.shape[0], kernels, chans, samples)\r\n",
        "\r\n",
        "        print('X_train shape:', X_train.shape)\r\n",
        "        print(X_train.shape[0], 'train samples')\r\n",
        "        print(X_val.shape[0], 'val samples')\r\n",
        "\r\n",
        "        print('X_train shape:', X_test.shape)\r\n",
        "        print(X_test.shape[0], 'test samples')\r\n",
        "\r\n",
        "        for id_d, d in enumerate(drop_out):\r\n",
        "          print(id_d)\r\n",
        "          \r\n",
        "          if model_name == 'DeepConvNet':\r\n",
        "            model = DeepConvNet(nb_classes=2, Chans = chans, Samples = samples, dropoutRate = d)\r\n",
        "          elif model_name == 'ShallowConvNet':\r\n",
        "            model = ShallowConvNet(nb_classes = 2, Chans = chans, Samples = samples, dropoutRate = d)\r\n",
        "          else:\r\n",
        "            raise ValueError('Model name wrong!')\r\n",
        "          \r\n",
        "\r\n",
        "          # compile the model and set the optimizers\r\n",
        "          model.compile(loss='categorical_crossentropy', optimizer='adam', \r\n",
        "                        metrics = ['accuracy'])\r\n",
        "\r\n",
        "          # count number of parameters in the model\r\n",
        "          numParams    = model.count_params() \r\n",
        "\r\n",
        "          # set a valid path for your system to record model checkpoints\r\n",
        "          checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=1, save_best_only=True)\r\n",
        "\r\n",
        "          # the syntax is {class_1:weight_1, class_2:weight_2,...}. Here just setting\r\n",
        "          # the weights all to be 1\r\n",
        "          class_weights = {0:1, 1:1}\r\n",
        "\r\n",
        "          history_temp = model.fit(X_train, Y_train, batch_size = 16, epochs = n_epochs, \r\n",
        "                              verbose = 2, validation_data=(X_val, Y_val),\r\n",
        "                              callbacks=[checkpointer], class_weight = class_weights)\r\n",
        "          \r\n",
        "          history[sub_idx,h_indx,id_d] = history_temp.history\r\n",
        "\r\n",
        "          print('\\n# Evaluate on test data')\r\n",
        "\r\n",
        "\r\n",
        "          results = model.evaluate(X_test, Y_test)\r\n",
        "          print('test loss, test acc:', results)\r\n",
        "\r\n",
        "          print(results)\r\n",
        "          acc_sub[sub_idx,h_indx,id_d] = results[1]\r\n",
        "          hyper_parameter_matrix[sub_idx,h_indx,id_d, :] = [x, h, d, results[1]]\r\n",
        "\r\n",
        "          K.clear_session()\r\n",
        "with open(outfname, 'wb') as handle:\r\n",
        "    pickle.dump(hyper_parameter_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)         \r\n",
        "with open(outfname_history, 'wb') as handle:\r\n",
        "    pickle.dump(history, handle, protocol=pickle.HIGHEST_PROTOCOL)      \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "X-ucg12zwqW3",
        "outputId": "6e19b62c-23f6-4c6f-d4ab-f720ae1d0817"
      },
      "source": [
        "\r\n",
        "h_cut = [30] # [only alpha, alpha + low beta, alpha + beta]\r\n",
        "drop_out = [0.25]\r\n",
        "n_epochs = 500\r\n",
        "model_name = 'DeepConvNet'  # ShallowConvNet or DeepConvNet\r\n",
        "kernels, chans, samples = 1, 12, 1536\r\n",
        "\r\n",
        "outfname = 'accuray_epochs' + str(n_epochs) + '_filter_' + str(h_cut) + '_drop_' + str(drop_out) + '_model_' + str(model_name) + '_patient_data.npy'\r\n",
        "\r\n",
        "hyper_parameter_matrix = np.load(outfname)\r\n",
        "\r\n",
        "ind = np.unravel_index(np.argmax(acc_sub, axis=None), acc_sub.shape)\r\n",
        "acc_sub\r\n",
        "hyper_parameter_matrix\r\n",
        "hyper_parameter_matrix[ind]\r\n",
        "reduced_acc_sub = np.mean(acc_sub, axis=0, keepdims=True)\r\n",
        "sem_acc = np.std(acc_sub, axis=0, keepdims=True)\r\n",
        "\r\n",
        "ind_reduced = np.unravel_index(np.argmax(reduced_acc_sub, axis=None), reduced_acc_sub.shape)\r\n",
        "print('Index is: {}'.format(ind_reduced[-1]))\r\n",
        "print('Hyperparameters are: {}'.format(hyper_parameter_matrix[ind_reduced]))\r\n",
        "print(hyper_parameter_matrix[ind_reduced])\r\n",
        "print(acc_sub)\r\n",
        "#files.download('accuray_epochs500_filter_[12, 18, 30]_drop_[0.25, 0.5]_model_ShallowConvNet_patient_data.npy') \r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b95c37de51e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0moutfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'accuray_epochs'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_filter_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_cut\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_drop_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop_out\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_model_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_patient_data.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mhyper_parameter_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munravel_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_sub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_sub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'accuray_epochs500_filter_[30]_drop_[0.25]_model_DeepConvNet_patient_data.npy'"
          ]
        }
      ]
    }
  ]
}